{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562681a9",
   "metadata": {},
   "source": [
    "# Crawler prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1715df",
   "metadata": {},
   "source": [
    "## Database initialization\n",
    "\n",
    "Before running the web crawler, we need to start a Docker container with a Postgre SQL database, which we will use as storage for our web crawler.\n",
    "\n",
    "To start the container, run the following command:\n",
    "```\n",
    "docker run --name postgresql-wier -e POSTGRES_PASSWORD=SecretPassword -e POSTGRES_USER=user -v $PWD/pgdata:/var/lib/postgresql/data -v $PWD/init-scripts:/docker-entrypoint-initdb.d -p 5432:5432 -d postgres:12.2\n",
    "```\n",
    "\n",
    "Then, run the `database.sql` script to intialize the crawldb database with all the necceseary tables and relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559afa24",
   "metadata": {},
   "source": [
    "## Seminar imports\n",
    "\n",
    "In here are the imports that are imported for our Seminar 1 needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aaeff46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Visualization library\n",
    "# import later\n",
    "\n",
    "# Database, concurent working\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Time\n",
    "from datetime import datetime\n",
    "\n",
    "# For getting the response code\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410dd76",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "These parameters determine how our web crawler will run.\n",
    "* `number_of_workers`: we determine how many concurent workers are running in parallel, speeding up the process of page retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2139ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a80c64",
   "metadata": {},
   "source": [
    "## URL seeds\n",
    "These pages will be used as a starting point for our crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df0bbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as starting pages for the crawler\n",
    "web_page_seeds = [\n",
    "    \"http://gov.si\",\n",
    "    \"http://evem.gov.si\",\n",
    "    \"http://e-uprava.gov.si\",\n",
    "    \"http://e-prostor.gov.si\"\n",
    "]\n",
    "\n",
    "# Temporary starting page\n",
    "WEB_PAGE_ADDRESS = \"http://evem.gov.si\"\n",
    "\n",
    "# Also put there pages into the frontier\n",
    "frontier = web_page_seeds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e6324",
   "metadata": {},
   "source": [
    "# Web crawler\n",
    "\n",
    "Below is the structure of the web crawler, as well as the code to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614b332",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Change `WEB_DRIVER_LOCATION` according to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c676485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# !!! CHANGE THIS DEPENDING ON YOUR MACHINE !!!\n",
    "WEB_DRIVER_LOCATION = \"C:/ManualInstalls/ChromeDriver/chromedriver.exe\"\n",
    "TIMEOUT = 5\n",
    "\n",
    "# If you comment the following line, a browser will show ...\n",
    "#chrome_options.add_argument(\"--headless\")\n",
    "chrome_options = Options()\n",
    "#Adding a specific user agent\n",
    "chrome_options.add_argument(\"user-agent=fri-wier-kj_lk_tm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25fea1",
   "metadata": {},
   "source": [
    "## URL frontier\n",
    "\n",
    "A list of URLs waiting to be parsed. Using the frontier we run the web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "036a3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_web_crawler():\n",
    "    while frontier:\n",
    "        # Get an url from the front of a frontier\n",
    "        url = frontier[0]\n",
    "        frontier.pop(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ----------------\n",
    "        # CHECK DUPLICATES\n",
    "        # ----------------\n",
    "        \n",
    "        # Check if it's a duplicate\n",
    "        # If it's a duplicate, skip to the next element, since we already did work on this url\n",
    "        if check_page_duplicates(url):\n",
    "            continue\n",
    "            \n",
    "        # Set site accecss time and convert it to SQL appropriate format\n",
    "        accessed_time = datetime.now().isoformat()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # --------------------\n",
    "        # PAGE INFO EXTRACTION\n",
    "        # --------------------\n",
    "        \n",
    "        try:\n",
    "            http_status_code = requests.head(url).status_code\n",
    "        except requests.ConnectionError:\n",
    "            # Determine what to do here; currently continue with the next item\n",
    "            continue\n",
    "        \n",
    "        # Get HTML from the url\n",
    "        html = download_and_render_page(url)\n",
    "        \n",
    "        # Parse the content of the webpage, to extract links, images etc.\n",
    "        urls, images_urls = extract_data(html)\n",
    "        \n",
    "        # Add urls to frontier\n",
    "        for url in urls:\n",
    "            frontier.append(url)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        # SITE\n",
    "        # ----\n",
    "        \n",
    "        # Check if the site is already present in the database, if it is, find it and get the id, otherwise create one\n",
    "        domain = extract_domain(url)\n",
    "        site_id = find_site(domain)\n",
    "        if site_id == -1:\n",
    "            robots_content = get_robots_content(domain)\n",
    "            sitemap = get_sitemap_content(url)\n",
    "            \n",
    "            insert_site(domain, robots_content, sitemap)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        # PAGE\n",
    "        # ----\n",
    "\n",
    "        page_id = insert_page(site_id, page_type_code, url, html, http_status_code, accessed_time)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        # LINK\n",
    "        # ----\n",
    "        \n",
    "        # Insert links into database\n",
    "        # TODO - figure out how to save when there's link to a page that doesn't exist in the database; -1 so far\n",
    "        for url in urls:\n",
    "            insert_link(page_id, find_page(url))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ---------\n",
    "        # PAGE DATA\n",
    "        # ---------\n",
    "        \n",
    "        # TODO finish\n",
    "        page_data = \"HTML\"\n",
    "        insert_page_data(code, page_id, data_type_code, page_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # -----\n",
    "        # IMAGE\n",
    "        # -----\n",
    "        \n",
    "        # Insert images into database; TODO finish the image data extraction\n",
    "        image_data = \"TEMP\"\n",
    "        for image in images_urls:\n",
    "            insert_image(page_id, filename, content_type, image_data, accessed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34f91c",
   "metadata": {},
   "source": [
    "## HTTP downloader and renderer\n",
    "\n",
    "Retrieves and renders a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff6da8",
   "metadata": {},
   "source": [
    "### Downloading and rendering a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc54cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_render_page(url):\n",
    "    chrome_options.add_argument(\"user-agent=fri-wier-kj_lk_tm\")\n",
    "    \n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    TIMEOUT = 5\n",
    "    time.sleep(TIMEOUT)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3ed38",
   "metadata": {},
   "source": [
    "## Data extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee7015b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    urls_array = []\n",
    "    images_array = []\n",
    "    \n",
    "    for link in soup.find_all('link'):\n",
    "        urls_array.append(link['href'])\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        urls_array.append(link['href'])\n",
    "    \n",
    "    # TODO do the extraction for the elements with an onclick attribute as well\n",
    "    \n",
    "    for link in soup.find_all('img'):\n",
    "        images_array.append(link['href'])\n",
    "    \n",
    "    return urls_array, images_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede458b1",
   "metadata": {},
   "source": [
    "### Site data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3963b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain name from the link\n",
    "def extract_domain(link):\n",
    "    split_link = link.split(\"/\")\n",
    "    return split_link[0] + \"//\" + split_link[2]\n",
    "\n",
    "\n",
    "\n",
    "# Get robots.txt content from current domain\n",
    "def get_robots_content(domain):\n",
    "    chrome_options.add_argument(\"user-agent=fri-wier-kj_lk_tm\")\n",
    "    \n",
    "    domain += \"/robots.txt\"\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(domain)\n",
    "    \n",
    "    TIMEOUT = 5\n",
    "    time.sleep(TIMEOUT)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_sitemap_content(domain):\n",
    "    chrome_options.add_argument(\"user-agent=fri-wier-kj_lk_tm\")\n",
    "    \n",
    "    # TODO - fix this, as the sitemap doens't neccesearily reside at this address\n",
    "    domain += \"/sitemap.xml\"\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(domain)\n",
    "    \n",
    "    TIMEOUT = 5\n",
    "    time.sleep(TIMEOUT)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Finding all the links in the page\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = []\n",
    "    \n",
    "    for link in soup.find_all('link'):\n",
    "        content.append(link['href'])\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        content.append(link['href'])\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return \", \".join(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8f999",
   "metadata": {},
   "source": [
    "## Duplicate detector\n",
    "\n",
    "Detects already parsed pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8f1883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_duplicates(url):\n",
    "    if find_page(url) == -1:\n",
    "        return false\n",
    "    return true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccdb9b",
   "metadata": {},
   "source": [
    "## Datastore\n",
    "\n",
    "Store the data and additional metadata used by the crawler. Here resides the logic used to access database and preform actions such as printing certain table's content, inserting into the said table, et cetera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d8c79",
   "metadata": {},
   "source": [
    "### Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "080a21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_site(domain, robots_content, sitemap_content):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.site (domain, robots_content, sitemap_content) VALUES ('\"\n",
    "                + domain + \"','\"\n",
    "                + robots_content + \"','\"\n",
    "                + sitemap_content + \"' RETURNING id);\")\n",
    "    \n",
    "    id = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Idk if this is needed, but why would we store more of the same domain (it's not mentioned in the instructions)\n",
    "def find_site(domain):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM crawldb.site WHERE domain='\" + domain + \"';\")\n",
    "    \n",
    "    site_id = -1\n",
    "    \n",
    "    # Check if array is empty, meaning we didn't find the site already present in the table\n",
    "    if cur.fetchall():\n",
    "        site_id = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return site_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4af8cd",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb49c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_image(page_id, filename, content_type, data, accessed_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.image (page_id, filename, content_type, data, accessed_time) VALUES (FOREIGN KEY REFERENCES crawldb.page\"\n",
    "                + page_id + \"),'\"\n",
    "                + filename + \"','\"\n",
    "                + content_type + \"','\"\n",
    "                + data + \"','\"\n",
    "                + accessed_time + \"');\")\n",
    "    \n",
    "    id = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_image(page_id, filename):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM crawldb.image WHERE page_id = '\" + page_id + \"' AND filename='\" + domain + \"';\")\n",
    "    \n",
    "    image_id = -1\n",
    "    \n",
    "    # Check if array is empty, meaning the site isn't already present in the table\n",
    "    if cur.fetchall():\n",
    "        image_id = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65857c",
   "metadata": {},
   "source": [
    "### Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "196c06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_page(site_id, page_type_code, url, html_content, http_status_code, accessed_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.page (site_id, page_type_code, url, html_content, http_status_code, accessed_time) VALUES (FOREIGN KEY REFERENCES crawldb.site\"\n",
    "                + site_id + \"),'\"\n",
    "                + page_type_code + \"','\"\n",
    "                + html_content + \"',\"\n",
    "                + http_status_code + \",'\"\n",
    "                + accessed_time + \"');\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_page(url):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM crawldb.page WHERE url='\" + url + \"';\")\n",
    "    \n",
    "    page_id = -1\n",
    "    \n",
    "    # Check if array is empty, meaning the site isn't already present in the table\n",
    "    if cur.fetchall():\n",
    "        page_id = cur.fetchone()[0]\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return page_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fc462",
   "metadata": {},
   "source": [
    "### Page data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b34b14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_page_data(page_id, data_type_code, data):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.page_data (page_id, data_type_code, data) VALUES ('\"\n",
    "                + page_id + \"','\"\n",
    "                + data_type_code + \"','\"\n",
    "                + data + \"');\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976c694",
   "metadata": {},
   "source": [
    "### Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8e7d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_type(code):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.page (code) VALUES ('\" + code + \"');\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36639bd1",
   "metadata": {},
   "source": [
    "### Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d141cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_link(from_page, to_page):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.link (page_id, data_type_code, data) VALUES (FOREIGN KEY REFERENCES crawldb.page(\"\n",
    "                + from_page + \"),FOREIGN KEY REFERENCES crawldb.page(\"\n",
    "                + to_page + \"));\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4966106",
   "metadata": {},
   "source": [
    "### Page type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e95ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_page_type(code):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO crawldb.page_type (code) VALUES ('\" + code + \"');\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1244ce",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Concurrent execution of web crawler using multiple workers as given by the `number_of_workers` variable. Stop by stopping the working cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52301c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ... executing workers ...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'initialize_web_crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-a839e606b809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n ... executing workers ...\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitialize_web_crawler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_web_crawler' is not defined"
     ]
    }
   ],
   "source": [
    "# Workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    print(f\"\\n ... executing workers ...\\n\")\n",
    "    for _ in range(number_of_workers):\n",
    "        executor.submit(run_web_crawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddb481",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially insert possibly otherwise useful code (like debugging code) later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01fbec",
   "metadata": {},
   "source": [
    "This is here just because I hate how `.ipynb` creates an empty cell after the last run one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
